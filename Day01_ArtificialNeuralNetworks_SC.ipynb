{"cells":[{"cell_type":"markdown","metadata":{"id":"OvfQ24FDETfi"},"source":["<img src=\"./images/cads-logo.png\" style=\"height: 100px;\" align=left>\n","<img src=\"./images/tf-logo-2.png\" style=\"height: 70px;\" align=right>\n","<img src=\"./images/keras-logo.png\" style=\"height: 50px;\" align=right>"]},{"cell_type":"code","source":["<img src=\"./images/cads-logo.png\" style=\"height: 100px;\" align=left>\n","<img src=\"./images/tf-logo-2.png\" style=\"height: 70px;\" align=right>\n","<img src=\"./images/keras-logo.png\" style=\"height: 50px;\" align=right>"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"t9xahTk9Fyqa","executionInfo":{"status":"error","timestamp":1727922782021,"user_tz":-480,"elapsed":479,"user":{"displayName":"Zuhair Nasir","userId":"00812776960596939130"}},"outputId":"ca1ec2a8-1af6-492c-da39-e14fbf275b17"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-1-d946427f5eca>, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-d946427f5eca>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <img src=\"./images/cads-logo.png\" style=\"height: 100px;\" align=left>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","metadata":{"id":"FgL41CGNETfj"},"source":["# Artificial Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"B9XWJSShETfk"},"source":["**Pre-requisites:**\n","\n","1. Python and Pandas Proficiency\n","2. Supervised Machine Learning\n","3. Basic Matrix Operations\n","4. Linear Alegbra, Calculus\n"]},{"cell_type":"markdown","metadata":{"id":"ZlmHKXy4ETfk"},"source":["## Course Outline\n","\n","1. [Introduction](#1.-Introduction) <br>\n","1. [Churn Prediction Problem](#2.-Churn-Prediction-Problem) <br>\n","1. [Components of Artificial Neural Networks](#3.-Components-of-Artificial-Neural-Networks) <br>\n","1. [Artificial Neural Networks Architecture](#4.-Artificial-Neural-Networks-Architecture) <br>\n","1. [Improving Neural Network Performance](#5.-Improving-Neural-Network-Performance)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4-Svt7DETfk"},"outputs":[],"source":["# !pip install --upgrade pip\n","# !pip install --upgrade numpy\n","# !pip install sklearn\n","# !pip install seaborn\n","# !pip install tensorflow\n","# !pip install -q git+https://github.com/tensorflow/docs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRLz6OgrETfl"},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ngyJCIc2ETfl"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"VRctd6QZETfm"},"source":["# 1. Introduction\n"]},{"cell_type":"markdown","metadata":{"id":"owhyZioYETfm"},"source":["The pursuit of **thinking machines**  having **human-like general intelligence** begun from 1943's Electronic Brain and dawned on 1969's XOR Problem. A new wave of interest in  Artificial Intelligence was seen in 1986 through the **Multi-layered Perceptron** employing **Backpropagation**, following the two-decade of *AI Winter*. (The people responsible for this are called *Backpropagandists*). To date, humanity enjoys various applications of Deep Learning as it became a niche for several groups of scientists, engineers, and ethusiasts.\n","\n","- Near-human-level image classification, speech recognition, handwriting transcription\n","- Improved machine translation, text-to-speech conversion\n","- Digital assistants such as Google Now and Amazon Alexa\n","- Near-human-level autonomous driving\n","- Improved ad targeting, as used by Google, Baidu, and Bing\n","- Improved search results on the web\n","- Ability to answer natural-language questions\n","- Superhuman Go playing\n","\n","Deep Learning was made possible by the increase in scale of the following technical forces:\n","\n","- **Hardware**: increase of computing power\n","- **Datasets** and **Benchmarks**: increase in data volume and well-thought benchmarks\n","- **Algorithms**: parallel and high-performace computing"]},{"cell_type":"markdown","metadata":{"id":"D4vZQfc0ETfm"},"source":["# 2. Churn Prediction Problem"]},{"cell_type":"markdown","metadata":{"id":"DT4ahfUJETfm"},"source":["Before going into the detail of an artificial neural network  mechanisms, let's construct a model that will predict if the cutomer churned (`1`)  or is retained (`0`) `./data/churn.csv`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmhlvnl1ETfm"},"outputs":[],"source":["# Question : Which type of machine learning technique should we employ to solve the problem above?\n","\n","# SC\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"s0CSnXyIETfm","executionInfo":{"status":"error","timestamp":1727921481483,"user_tz":-480,"elapsed":15247,"user":{"displayName":"Suria Kumar Karuppannan","userId":"05283078601985944732"}},"outputId":"969cd927-bab6-4226-e70d-0d855b496bb8"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow_docs'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-61a1f4dd8959>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_docs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfdocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_docs'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import functools\n","\n","import numpy as np\n","np.random.seed(42)\n","\n","import os\n","import tempfile\n","import datetime\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","plt.rcParams['figure.figsize'] = (8, 6)\n","colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n","\n","from plot_utils import compareValLoss, compareValAcc, plot_metrics, plot_cm, plot_roc\n","\n","import sklearn\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import confusion_matrix\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, regularizers\n","import tensorflow_docs as tfdocs\n","import tensorflow_docs.plots\n","import tensorflow_docs.modeling\n","\n","print(tf.__version__)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"s8uY5A8qEliH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ngP7qBYETfn"},"outputs":[],"source":["# Load dataset\n","data_path = './data/churn.csv'\n","data = pd.read_csv(data_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIHvL10mETfn"},"outputs":[],"source":["# Identify datatypes\n","data.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"5xhdw7FpETfn"},"outputs":[],"source":["# Identify unique values in data.Geography\n","data.Geography.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"en_F93EtETfn"},"outputs":[],"source":["data.head()"]},{"cell_type":"markdown","metadata":{"id":"cgbCzKE7ETfn"},"source":["Notice that `Surname`, `Geography`, and `Gender` all have  **`object`** datatypes. These are **categorical** data in string format. We **drop** the  `RowNumber`, `CustomerID`, and `Surname`. We transfrom **`Geography`** and **`Gender`** entries to datatypes that are **readable** to an artificial neural network. Know that artificial neural networks are simply **mathematical functions** and are implemented in codes. `Gender` data, when converted to numerical form, becomes *binary* data. `Geography` on the other hand has cardinality of `3`. It should be one-hot encoded.\n","\n","Other categorical data are `HasCrCard` and `IsActiveMember`, although they are already in binary format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rLv_iqooETfn"},"outputs":[],"source":["# Drop data.Surname\n","data = data.drop(columns=['RowNumber','CustomerId','Surname'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNcMPVuwETfn"},"outputs":[],"source":["# Tell pandas that data.Gender is a categorical data, instead of object\n","data['Gender'] = pd.Categorical(data.Gender)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OG-K7vh0ETfn"},"outputs":[],"source":["# Convert categorical data to numbers\n","data['Gender'] = data.Gender.cat.codes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQZqjZoDETfo"},"outputs":[],"source":["# One-hot encode the data.Geography\n","data = pd.get_dummies(data,prefix=[''])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VSANf2UwETfo"},"outputs":[],"source":["data.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"apNSImoJETfo"},"outputs":[],"source":["sns.countplot(x='Exited', data=data, palette='hls')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQqtO9ooETfo"},"outputs":[],"source":["count_retain = len(data[data['Exited']==0])\n","count_churn = len(data[data['Exited']==1])\n","percent_retain = count_retain/(count_retain+count_churn)\n","print(\"Percent of Retained Customers\", percent_retain*100)\n","percent_churn = count_churn/(count_retain+count_churn)\n","print(\"Percent of Churned Customerns\", percent_churn*100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJAKsK_UETfo"},"outputs":[],"source":["# Question : What can you say about our dataset, given `Exited` is the target?\n","\n","# SC\n"]},{"cell_type":"markdown","metadata":{"id":"XV7OCMt0ETfo"},"source":["We create **`train`**, **`test`** , and **`val`** sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-4rbyg8ETfo"},"outputs":[],"source":["# Split and shuffle our dataset.\n","train_data, test_data = train_test_split(data.copy(), test_size=0.2)\n","train_data, val_data = train_test_split(train_data, test_size=0.2)\n","\n","# Form np arrays of labels and features.\n","y_train = np.array(train_data.pop('Exited'))\n","bool_y_train = y_train != 0\n","y_val = np.array(val_data.pop('Exited'))\n","y_test = np.array(test_data.pop('Exited'))\n","\n","X_train = np.array(train_data)\n","X_val = np.array(val_data)\n","X_test = np.array(test_data)"]},{"cell_type":"markdown","metadata":{"id":"hrtwvXvCETfo"},"source":["We normalize our input features using `StandardScaler()`. This will set the mean to `0` and standard deviation to `1`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gMfohh51ETfo"},"outputs":[],"source":["# Question : What does StandardScaler do to our data?\n","\n","# SC\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwwu-lWiETfo"},"outputs":[],"source":["scaler = StandardScaler()\n","\n","X_train = scaler.fit_transform(X_train)\n","X_val = scaler.transform(X_val)\n","X_test = scaler.transform(X_test)\n","\n","X_train = np.clip(X_train, -5, 5)\n","X_val = np.clip(X_val, -5, 5)\n","X_test= np.clip(X_test, -5, 5)\n","\n","\n","print('Training labels shape:', y_train.shape)\n","print('Validation labels shape:', y_val.shape)\n","print('Test labels shape:', y_test.shape)\n","\n","print('Training features shape:', X_train.shape)\n","print('Validation features shape:', X_val.shape)\n","print('Test features shape:', X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"huyTJuHQETfo"},"source":["### 2.1. Logistic Regression on `sklearn`\n","\n","Let's deal with the problem by first using the `Logistic Regression` class of `sklearn`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y-Ee51ysETfp"},"outputs":[],"source":["# Instantiate an sklearn model for logistic regression\n","model_lr = LogisticRegression()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5aNUbl7hETfp"},"outputs":[],"source":["# Start training\n","model_lr.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X7OT2MrgETfp"},"outputs":[],"source":["# Use trained model to predict on test dataset\n","y_pred = model_lr.predict(X_test)\n","print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(model_lr.score(X_test, y_test)))"]},{"cell_type":"markdown","metadata":{"id":"EOIbDGn9ETfp"},"source":["### 2.2. Logistic Regression with Neural Networks\n","\n","Below we try to solve the same problem, but this time we make use of an Artificial Neural Network. An example of instantiating a model object with the `Keras` API is given. Several `layers` are added to the `modelA`. We will discuss each line thoroughly as we go along the discussion. For now, take a look at how `modelA` is constructed."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"6LyhmpwcETfp"},"outputs":[],"source":[" # Instantiate modelA\n","modelA = keras.Sequential([\n","    tf.keras.layers.Dense(2, activation='relu', input_shape=[len(X_train[0])]),\n","    tf.keras.layers.Dense(2, activation='relu'),\n","    tf.keras.layers.Dense(1,   activation='sigmoid')\n","\n","  ])\n","\n","# Compile modelA\n","modelA.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","# Prepare training modelA log storage\n","logdirA = os.path.join(\"logs\",\"modelA\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callbackA = tf.keras.callbacks.TensorBoard(logdirA, histogram_freq=1)\n","\n","\n","# Train modelA\n","historyA = modelA.fit(X_train, y_train,\n","                    epochs=20,\n","                    batch_size=100,\n","                    validation_data=(X_val, y_val),\n","                    verbose=1,\n","                    callbacks=[tensorboard_callbackA])\n","\n","\n","# Evaluate modelA\n","modelA.evaluate(X_test, y_test)\n","\n","# Use trained modelA to predict on test dataset\n","modelA.predict(X_test)\n"]},{"cell_type":"markdown","metadata":{"id":"udLlXuTVETfp"},"source":["# 3. Components of Artificial Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"98kw_S-uETfp"},"source":["### 3.1. Neural Network Attributes"]},{"cell_type":"markdown","metadata":{"id":"375zXT0jETfp"},"source":["Each of the layers in the neural network is a **tensor operation (e.g. addition, multiplication, etc.)** applied to tensors of numeric data.\n","\n","```python\n","    modelA = keras.Sequential([\n","        tf.keras.layers.Dense(24, activation='relu', input_shape=[len(X_train.keys())]),\n","        tf.keras.layers.Dense(24, activation='relu'),\n","        tf.keras.layers.Dense(1,  activation='sigmoid')\n","      ])\n","```\n","These layers are stacked on top of each other in a **`Sequential`** manner as shown below. Each circle is considered a **`neuron`** OR **`unit`**."]},{"cell_type":"markdown","metadata":{"id":"DDzxl94lETfq"},"source":["<img src=\"./images/nn_bias_hidden.png\" style=\"height: 300px;\" align=left>"]},{"cell_type":"markdown","metadata":{"id":"rQkkD_tEETfq"},"source":["Notice that all neurons/units are connected to each one of the preceding layer. A layer with this configuration is called a **Fully-connected Layer** OR a **Densely-Connected Layer**, hence the call `tf.keras.layers.Dense`."]},{"cell_type":"markdown","metadata":{"id":"BNsPkwiFETfq"},"source":["Take for example this one layer and its output:\n","\n","```python\n","tf.keras.layers.Dense(24, activation='relu')`\n","```\n","The input data is transformed by the layer as follows:\n","\n"," `(3.2) output = relu(dot(W, input) + b)`\n","\n","We intepret this function as one that feeds on a 2D tensor and returns a 2D tensor. Here `W` is a 2D tensor and `b` is a vector.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHB5fnfpETfq"},"outputs":[],"source":["# Question : Identify the tensor operations used in equation (3.2)\n","\n","# SC\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LQSvr3imETfq"},"source":["In the corresponding network graph above, **`X`** correspond to the `features`. In our churn data example there are `12` columns corresponding to `12 features`. Therefore, there are `12 units` in the input layer. There are `24 units` for the first hidden layer, another `24 units` for the second hidden layer and `1 unit` for the output layer. There are additional `3 units` of `b`, one for each hidden and output layer.\n","\n","**`W`** and **`b`** are tensor attributes of the layer. These attributes are called **weights** OR **trainable attributes**. Each arrow you see carries a weight. Everytime the neural network is exposed to new data, these attributes are updated. They store all the information **learned** by the network during **training**.  Imagine the **`W`** (**kernels**) as knobs or buttons that we can tweak to correctly predict the churn of customers. On the otherhand, **`b`** (**bias**) does NOT interact with the input data. Furthermore, it has a weight = `1`. You'll often see `b` written as `Wo`. It is a tensor of constant values that gives us some data-independent prefences of a class over the other."]},{"cell_type":"markdown","metadata":{"id":"n6EwsLjfETfq"},"source":["### 3.2. Counting Trainable Parameters for Fully Connected Layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zWpKwYCPETfq"},"outputs":[],"source":["# Question : In our churn prediction neural network, how many are our trainable parameters?\n","# Hint: Connections between layers + bias connections in each layer\n","\n","# SC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNB2wDnoETfq"},"outputs":[],"source":["# Question : How many trainable parameters are there if we have 3 feature columns, a hidden layer of 5 units, and output layer with 3 units?\n","\n","\n","# SC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RmuPoedVETfq"},"outputs":[],"source":["# Question : How many trainable parameters are there if we have 10 feature columns, a hidden layer of 2 units, and output layer with 1 units?\n","\n","\n","# SC"]},{"cell_type":"markdown","metadata":{"id":"y_7vJ66hETfq"},"source":["### 3.3. Feed Forward"]},{"cell_type":"markdown","metadata":{"id":"5CFcvBTvETfq"},"source":["Recall that we defined above `EPOCHS = 10`. An epoch is also called a **training loop**.\n","\n","Based from the previous network graph, the information travels from the input layer to the output layer and the process follows the following steps\n","<br>\n","**Feedforward**:\n","\n","1. Gather the training samples `X` and corresponding targets `y`.\n","2. Calculate the `dot` product of `X` and `W` :  `dot(W, X) + b`\n","3. Pass the result to an **activation function** :  `relu(dot(W,X) + b)`"]},{"cell_type":"markdown","metadata":{"id":"vi3drQErETfq"},"source":["### 3.3.1 Activation Functions"]},{"cell_type":"markdown","metadata":{"id":"Yntnxm7mETfq"},"source":["Going back to the network we've just constructed:\n","\n","```python\n","    tf.keras.layers.Dense(2, activation='relu', input_shape=[len(train_dataset.keys())]),\n","    tf.keras.layers.Dense(2, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","```\n","\n","The `relu()` and `sigmoid()` are examples of **Activation Functions**.\n","<br>\n","We show below how each activation function processes input data."]},{"cell_type":"markdown","metadata":{"id":"kcekP-9GETfr"},"source":["#### 3.3.1.1. Rectified Linear Unit - RELU\n","\n","The RELU activation function is thresholded at 0. If `x<0`, it is \"squashed\" to zero. If `x>0`, it follows a line with `slope=1`. It’s NOT Linear, but it’s used to introduce linearity. It provides the same benefits as Sigmoid,\n","but with better performance.\n","\n","**When to use:** Can be used only in the hidden layers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trXB0n2TETfr"},"outputs":[],"source":["def relu(z):\n","    return np.maximum(z, 0)\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","input_ = np.linspace(-10, 10, 100)\n","plt.plot(input_, relu(input_), c=\"r\")\n","plt.title(\"ReLU Function\")"]},{"cell_type":"markdown","metadata":{"id":"QrbFaY7vETfr"},"source":["#### 3.3.1.2. Sigmoid\n","\n","It takes a real-valued number and \"squashes\" it into range between 0 and 1. Large negative numbers become 0 and large positive numbers become 1.\n","\n","**When to use:** If we have to predict the probability of observation.\n","<br>\n","**Note:** The probability of anything exists only between (0 to 1)."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"_3ytnFzCETfr"},"outputs":[],"source":["# define the sigmoid function\n","def sigmoid(z):\n","    return 1/(1+np.exp(-z))\n","\n","input_ = np.linspace(-10, 10, 100)\n","plt.plot(input_, sigmoid(input_), c=\"r\")\n","plt.title(\"Sigmoid Function\")"]},{"cell_type":"markdown","metadata":{"id":"UYBdhbSBETfr"},"source":["#### 3.3.1.3. Softmax\n","\n","Generalization of the Sigmoid function.\n","\n","**When to use:** If we have a multiclass problem.\n","<br>\n","**Note:** The sum of probabilities of observation is 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMAnIWkAETfr"},"outputs":[],"source":["# define a softmax function\n","def softmax(x):\n","    e_x = np.exp(x-np.max(x))\n","    return e_x /e_x.sum()\n","\n","input_ = np.linspace(-10, 10, 100)\n","plt.plot(input_, softmax(input_), c=\"r\")\n","plt.title(\"Softmax Function\")\n"]},{"cell_type":"markdown","metadata":{"id":"-EF_df7zETfr"},"source":["#### 3.3.1.4. Hyperbolic Tangent\n","\n","Negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\n","The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).\n","\n","**When to use:** When doing a classification between `2` classes.\n","<b>"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"C1MrNgerETfr"},"outputs":[],"source":["def tanh(z):\n","    return (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n","\n","input_ = np.linspace(-10, 10, 100)\n","plt.plot(input_, tanh(input_), c=\"r\")\n","plt.title(\"Hyperbolic Tangent\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PuAIEpIQETfr"},"outputs":[],"source":["# Question : What happens to our network if we do not use non-linear Activation Functions?\n","\n","\n","# MC\n","# Tensor operations reduce to  y = dot(W, input) + b.\n","# Network collapses to linear and is not able to capture non-linearity."]},{"cell_type":"markdown","metadata":{"id":"FIn3jwxiETfr"},"source":["Now that we have discussed the forward propagation of information and activation functions, we are now ready to tackle the following steps:\n","\n","4. Compute the **cost of the network, `J`** on the batch, a measure of the difference between `y_pred` and `y`.\n","5. Using **`Gradient Descent`**, find the values of `W` and `b` that minimizes the cost `J`.\n","6. Update all`W` and `b` simultaneously."]},{"cell_type":"markdown","metadata":{"id":"iALC6odDETfr"},"source":["### 3.4. Loss Function (Objective Function)"]},{"cell_type":"markdown","metadata":{"id":"ViYjNedgETfr"},"source":["`W` and `b` are intialized with small random values (*random initialization*). The initial outputs or representations are rarely useful. These weights are gradually adjusted based from the feedback signal. We repeat Feedforward and Backpropagation in a loop until we get `y_pred` that is close enough to `y`. The network learns to map the input to correct target (labels). This process of adjustment is called **training**.\n","\n","We will now discuss the rest of the gears of a neural network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5io5IjJETfs"},"outputs":[],"source":["# Question 6: What type of machine learning (Supervised or Unsupervised) was described by the steps above?\n","\n","# MC\n","# Supervised Machine Learning"]},{"cell_type":"markdown","metadata":{"id":"yeAmOj0IETfs"},"source":["The **Loss Function** determines how good our predicted label `y_pred` with respect to the true label `y` for a **single training example**. That means we want to minimize this loss. A usual form we know is half of the square difference between the predicted value `y_pred` and true value `y`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgIn-R-TETfs"},"outputs":[],"source":["# Define a sum of squared error function\n","def ss_loss(y, y_pred):\n","    return 1/2*np.sum((y - y_pred)**2)\n","\n","y = np.array([0., 0., 1., 1.])\n","y_pred = np.array([1., 1., 1., 0.])\n","\n","print(\"Sum of Squared Losses:\", ss_loss(y,y_pred))"]},{"cell_type":"markdown","metadata":{"id":"46LxKESSETfs"},"source":["The image below shows an example plot of `W0` and `W1`. The height shown by the solid black arrow indicate the error (loss) between `y_pred` and `y`. Our aim is to find a combination of values of `W0` and `W1` such that the error is small enough.\n","\n","<img src=\"./images/nn_loss.png\" style=\"height: 300px;\" align=left>"]},{"cell_type":"markdown","metadata":{"id":"dpYAsFJWETfs"},"source":["Image Credit ([Source](https://www.eecs.wsu.edu/~cook/dm/lectures/l5/node10.html))"]},{"cell_type":"markdown","metadata":{"id":"w305nKhuETfs"},"source":["While the Loss Function determines the error for one training example, the **Cost Function** on the other hand measures how well the network is doing for the **entire training set**.\n","\n","Recall the line above where we compiled our model.\n","\n","```python\n","model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","```\n","We used the `BinaryCrossentropy`. An example of how this function computes the loss is given below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJjpMd2hETfs"},"outputs":[],"source":["bce = tf.keras.losses.BinaryCrossentropy()\n","loss = bce([0., 0., 1., 1.], [1., 1., 1., 0.])\n","print('Loss: ', loss.numpy())"]},{"cell_type":"markdown","metadata":{"id":"6lTXM18RETfs"},"source":["Which loss function to use is highly dependent on the problem at hand. For instance: <br>\n","- **mean- squared error** for a **regression problem**\n","- **binary crossentropy** for a **two-class classification problem** <br>\n","- **categorical crossentropy** for a **multi-class classification problem** <br>\n","- **connectionist temporal classification (CTC)** for a **sequence-learning problem**\n","\n","\n","\n","[You may check-out different `tf.keras.losses` functions and usage.](https://www.tensorflow.org/api_docs/python/tf/keras/losses)"]},{"cell_type":"markdown","metadata":{"id":"p33tzd2pETfs"},"source":["### 3.5. Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"jfDlq_uvETfs"},"source":["**I. In our churn prediction network, how do we know if we should increase or decrease each of the `35` coefficients and by how much?**\n","<br> One way is to tweak `1` of the coefficients and freeze the remaining `34`. Say the one we are tweaking has an initial value `W=0.2`. After one loop of passing the batch to the network, we have `loss=0.5`. We update the value to `W=0.3` and do a second feedforward. This time `loss=0.6`. We then update the coefficient to `W=0.35` and the loss drops to `loss=0.2`. This means we need to change `W` by `-0.05` to minimize the `loss`. If we do the same to the other `34` coefficients, we'll find it very inefficient, expensive, and time-consuming. Later we're to deal with networks having *thousands to millions* of trainable parameters! <br>  \n","\n","**II. How do we make our entire network from having high loss to low loss?** <br>\n","All of the tensor operations in our previous network are **differentiable** having tractable **gradients**. We can solve for the **derivative** of loss with respect to the coefficients, i.e. compute how the loss changes with respect to the change of weights, and update the weights we feed into the network accordingly.\n","All along we are solving an **Optimization** problem. One algorithm that solves such problem is called **Gradient-based Optimization** OR **Gradient Descent**.  <br>"]},{"cell_type":"markdown","metadata":{"id":"o_OAMQL4ETfs"},"source":["<img src=\"./images/nn_local.png\" style=\"height: 300px;\" align=left>"]},{"cell_type":"markdown","metadata":{"id":"eWw_0hLLETfs"},"source":["For simplicity, instead of `(3.2)`,  we use equation `(3.3)` to get the `y_pred`. <br>\n","**` (3.3) y_pred = dot(W, x) + b`** <br>\n","\n","The loss value is a function of both `y_pred` and `y`.  <br>\n","**` (3.4) loss_value = loss(y_pred,y)`** <br>\n","\n","If `x` and `y` are frozen, the loss value is a function of `W`. This means we can map the losses from the coefficients `W`. <br>\n","**` (3.5) loss_value = L(W)`** <br>\n","\n","The derivative of `L` in the point `Wo` is a tensor gradient, having the same shape as `W`: <br>\n","**` (3.6) derivative(L)(Wo) = gradient(L)(Wo)`**\n","\n","**`gradient(L)(Wo)`** can be interpreted as tensor describing the **curvature** of `f(W)` around `Wo`.\n","\n","**III. How do we update `Wo` to achieve lowest loss?** <br>\n","If **`gradient(L)(Wo)`** is NEGATIVE, a small positive change in **`Wo`** causes a DECREASE in **`L`**. <br>\n","If **`gradient(L)(Wo)`** is POSITIVE, a small positive change in **`Wo`** causes a INCREASE in **`L`**. <br>\n","The magnitude or absolute value of **`gradient(L)(Wo)`** tells us how quickly `L` will change.\n","\n","We move **`Wo`** a little in the OPPOSITE direction of **`gradient(L)(Wo)`**, i.e. if the gradient is positive, we move `Wo` towards lower values; if the gradient is negative, we move `Wo` towards positive values until we reach the optimum value of `Wo` that will give the lowest loss `L`. In equation form: <br>\n","**`(3.7) Wo -= learning_rate*gradient(L)(Wo)`**\n","\n","We don't want to get too far from the initial value `Wo` and the `learning_rate` helps manage that condition."]},{"cell_type":"markdown","metadata":{"id":"V2oDWw7UETfs"},"source":["<img src=\"./images/nn_learning_rate.png\" style=\"height: 300px;\" align=left>"]},{"cell_type":"markdown","metadata":{"id":"bq2Gs0FUETft"},"source":["### 3.5.1. Learning Rate\n","\n","The choice of **`learning_rate`** or step-size has implications to the learning of the network. As shown in the figure above, we opt to take a step-size that  will help the training to converge. It follows that if the step-size is too small, training will take a long time before the network reaches the lowest loss possible (global minimum). A step-size that is too large may cause the network to NOT converge at all."]},{"cell_type":"markdown","metadata":{"id":"yDxk4EqQETft"},"source":["### 3.6. Backpropagation"]},{"cell_type":"markdown","metadata":{"id":"xEVLWK7wETft"},"source":["We've considered above a case of data flowing in a forward pass without an activation function : given `x` we calculate `y_pred = dot(Wo,x)`, compute the loss `L`, solve for the tensor `gradient(L)(Wo)`, and update `Wo` accordingly. Now we consider having an activation function as intermediate layer that feeds on the input `x`.\n","\n","**I. How do we update `W` of the network?**"]},{"cell_type":"markdown","metadata":{"id":"7VDwfR02ETft"},"source":["<img src=\"./images/nn_feedforward.png\" style=\"height: 300px;\" align=left>"]},{"cell_type":"markdown","metadata":{"id":"74CDKLjdETft"},"source":["### 3.6.1. Chain Rule\n"]},{"cell_type":"markdown","metadata":{"id":"YBVVRsOFETft"},"source":["Assuming that the activation function used above is RELU, the network is described by the following:\n","\n","**`f(W) = relu(dot(W,X) + b)`** <br>\n","**`f(W) = y_pred`** <br>\n","**`loss_value = loss(y_pred,y)`** <br>\n","\n","But this time **`L`** is not explicitly a function of **`W`**. Instead, it is a function **`relu(z)`**, where **` (3.8) z=dot(W,X) + b`**.\n","\n","**`(3.9) loss_value = L(relu)`** <br>\n","**`(3.10) derivative(L)(relu) = gradient(L)(relu)`** <br>\n","\n","We can map the value of `relu` with the function `z`\n","\n","**`(3.11) relu_value = relu(z)`**<br>\n","**`(3.12) derivative(relu)(z) = gradient(relu)(z)`** <br>\n","\n","We see now that we can map the values of `z` with `W` <br>\n","**`(3.13) z_value = z(W)`**<br>\n","**`(3.14) derivative(z)(W) = gradient(z)(W)`** <br>\n","\n","**Chain rule** states that we can compute for the derivative of `L` with respect to `W` by:  <br>\n","\n","**`(3.15) derivative(L)(W)  =  derivative(L)(relu) * derivative(relu)(z) * derivative(z)(W) * `**\n","\n","And so the value for the gradient is: <br>\n","**`(3.16) gradient(L)(W)  =  gradient(L)(relu) * gradient(relu)(z) * gradient(z)(W) * `**\n","\n","We can now update **`W`** as follows: <br>\n","**`(3.17) W -= learning_rate*gradient(L)(W)`**"]},{"cell_type":"markdown","metadata":{"id":"ufXRbODwETft"},"source":["### 3.6.2. Mini-batch Stochastic Gradient Descent\n","\n","In cases where we have tens of millions of parameters/coefficients (like in high-definition image data), we may want to run our training in `mini-batches`. Our pipeline will then go through the following steps:\n","\n","1. Draw a batch of training samples `X` and corresponding targets `y`.\n","2. Run the network on `X` to obtain predictions` y_pred`.\n","3. Compute the loss of the network on the batch, `L(y_pred,y)`.\n","4. Compute the gradient of the loss with regard to the network’s parameters `gradient(L)(W) `  (backpropagation).\n","5. Move the parameters a little in the opposite direction from the gradient—`W -= step * gradient`- reducing the loss on the batch a bit.\n","\n","\n","Instead of the usual `Gradient Descent`, here we use `Mini-batch Stochastic Gradient Descent (SGD)`. The `stochastic` term comes from the fact that each batch is chosen in `random` manner."]},{"cell_type":"markdown","metadata":{"id":"w-jbFV-0ETft"},"source":["### 3.6.3. Optimizers\n","\n","We just saw that we can use  optimizers such as `Gradient Descent` and its variant `Mini-batch Stochastic Gradient Descent`. Other optimizers offered by Tensorflow [can be found in this page.](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n"]},{"cell_type":"markdown","metadata":{"id":"L3VaGDclETft"},"source":["### 3.6.4. The Vanishing Gradient Problem\n","\n","The gradient in equation **`3.16`** tends to become smaller and smaller as it feedbacks to the layers of a very deep network. This is the **_Vanishing Gradient Problem_**. When **`gradient(L)(W)`** becomes very small, it follows that the weight given by equation **`3.17`** doesn't update. Thus, the network stops learning.\n"]},{"cell_type":"markdown","metadata":{"id":"55yaiIpIETft"},"source":["# 4. Artificial Neural Networks Architecture"]},{"cell_type":"markdown","metadata":{"id":"qn0nWszvETft"},"source":["### 4.1. Network Layers"]},{"cell_type":"markdown","metadata":{"id":"sgGZhKfHETft"},"source":["We are now ready to discuss thoroughly the binary classifier model we built above. <br>\n","The layers below, when combined, become a **_network_** or **_model_**. Here we are still preparing our network prior any training."]},{"cell_type":"markdown","metadata":{"id":"fYgLINCrETft"},"source":["```python\n","    modelA = keras.Sequential([\n","        tf.keras.layers.Dense(2, activation='relu', input_shape=[len(X_train[0])]),\n","        tf.keras.layers.Dense(2, activation='relu'),\n","        tf.keras.layers.Dense(1,   activation='sigmoid')\n","\n","      ])\n","    ```"]},{"cell_type":"markdown","metadata":{"id":"a7eDwZyDETft"},"source":[" The type of layer to be used should be appropriate to `tensor` format and datatype. Below are real-world examples of  `tensor` datatypes. <br>\n","\n","- Vector Data: **`2D tensors`** with shape `(samples, features)`\n","- Timeseries or Sequence Data: **`3D tensors`** with shape `(samples, timesteps, features)`\n","- Images: **`4D tensors`**  with shape `(samples,height,width,channels)`\n","- Video: **`5D tensors`**  with shape `(samples, frames, height, width, channels)`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zWPPM7-sETfu"},"outputs":[],"source":["# Question 6: Identify the tensor data type and shape of our churn train dataset.\n","\n","# SC"]},{"cell_type":"markdown","metadata":{"id":"QyfDUb_8ETfu"},"source":["Another way of looking at stacking **compatible** layers is creating a multi-stage data transformation pipeline. The network should follow a strict layer compatibility since one layer will only process an input tensor of certain shape and output a tensor that also has a certain shape. Again, take a look at this example: <br>\n","\n"," **` (4.1) tf.keras.layers.Dense(2, activation='relu', input_shape=[len(X_train[0])])`**\n","\n","Note that every input data has a **`input_shape=[len(X_train[0])] =[12]`**. <br>\n","We are creating a layer that accepts a `2D tensor` where the first dimension is `(12,)`. <br>\n","This layer outputs a first dimension transformed from `(12,)` to `(2,)`.\n","\n","The second layer connected to the layer above, therefore, should accept a `2D tensor` having a first dimension of `(2,)`. <br>\n","\n","**` (4.2) tf.keras.layers.Dense(2, activation='relu')`**\n","\n","Keras layers are dynamically compatible. We need not define the input shape for the second layer because it already assumes that the input shape is the same as the shape of the layer output preceding it. The `summary` class provides a useful look-up of each layer's output shape as well as the total number of trainable parameters.  "]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"seKonZXvETfu"},"outputs":[],"source":["# Print the model summary\n","modelA.summary()"]},{"cell_type":"markdown","metadata":{"id":"6HKntHRCETfu"},"source":["### 4.2. Network Compilation\n","\n","During the compilation, we configure the learning process. Here we specify our **`optimizer`**. It determines how the network (trainable parameters) will be updated by a variant of stochastic gradient descent. We also define the **`loss (objective) function`** which is used as a feedback signal for learning the weight tensors `W`."]},{"cell_type":"markdown","metadata":{"id":"73uEu8sFETfu"},"source":["```python\n","    modelA.compile(loss='binary_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])\n","    ```"]},{"cell_type":"markdown","metadata":{"id":"bA1j5MpDETfu"},"source":["### 4.3. Network Training\n","\n","The network will start to iterate on the training data `10` times. One epoch is one iteration over all the training data. At each iteration, the network will compute the gradients of the weights with regard to the loss on the batch, and update the weights accordingly. After these `10` epochs, the network will have performed `350` gradient updates (`35` per epoch), and the loss of the network will be sufficiently low that the network will be capable of predicting the churn. Below, we now define the `batch_size=100`. The network will start to iterate on the `train_dataset` in `100` mini-batches, `10` times over. The training results is stored in a container call `historyA`."]},{"cell_type":"markdown","metadata":{"scrolled":true,"id":"H75NK69FETfu"},"source":[" ```python\n","    historyA = modelA.fit(X_train, y_train,\n","                        epochs=20,\n","                        batch_size=100,\n","                        validation_data=(X_val, y_val),\n","                        verbose=1,\n","                        callbacks=[tensorboard_callbackA])\n","```"]},{"cell_type":"markdown","metadata":{"id":"XyRCUXfmETfu"},"source":["Tensorflow offers  **TensorBoard** , an elegant visualization toolkit that allows us to track the experimentation results. We added more lines to our code above: we first define **`logdirA`** which will house all our training **`logsA`**.\n","Later we'll see how we can use this historical logs for future experiments. Second, we set our training **`callback`** to **`tensorboard_callbackA`**."]},{"cell_type":"markdown","metadata":{"id":"kA7I3wayETfu"},"source":["  ```python\n","logdirA = os.path.join(\"logsA\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callbackA = tf.keras.callbacks.TensorBoard(logdirA, histogram_freq=1)\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"IYxQQQgyETfu"},"source":["We start the **TensorBoard** within the notebook using magics. We can even start it *prior* training to monitor progress. This extension will be very useful once we deal with high dimensional tensor data where the output of hidden layers is not intuitive."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"XLB6K4mQETfu"},"outputs":[],"source":["%tensorboard --logdir=logs"]},{"cell_type":"markdown","metadata":{"id":"95xiwUcDETfu"},"source":["### 4.4. Network Evaluation\n","\n","We check how our model fair with the `X_test` and `y_test`. The `evaluate` class always have the first element `[0]` as  **`loss`**. You'll see later that we can add and evaluate on more metrics other than `loss` and `accuracy`."]},{"cell_type":"markdown","metadata":{"scrolled":true,"id":"fO0Qs8LyETfu"},"source":[" ```python\n","modelA.evaluate(X_test, y_test)\n","loss = modelA.evaluate(X_test, y_test)[0]\n","```"]},{"cell_type":"markdown","metadata":{"id":"V5ZHajeDETfu"},"source":["### 4.5. Network Prediction\n","\n","With the trained model, we can now use it to predict new samples."]},{"cell_type":"markdown","metadata":{"id":"lTw6eZ9METfv"},"source":["```python\n","modelA.predict(X_test)\n"," ```\n"]},{"cell_type":"markdown","metadata":{"id":"ESmfiRIoETfv"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"18EE7itFETfv"},"source":["## Exercise 1:"]},{"cell_type":"markdown","metadata":{"id":"CMeSXe6WETfv"},"source":["\n","Go back to the churn dataset. Construct your another network that will predict **`Exited`**.\n","\n","1. Make use of the previously-prepared `X_train`, `y_train`, `X_test` and `y_test`.\n","\n","2. Construct the model such that you have the following layers stacked in `Sequential` manner. Name it **`modelB`**.\n","\n","` Dense, 32 units, relu\n"," Dense, 32 units, relu\n"," Dense, 2 unit, softmax\n","  `\n","3. How many trainable parameters/coefficients do you have? Compare it with the `modelA` we created above.\n","\n","4. Compile your model using the following hyperparameters: <br>\n","\n","```python\n","        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","        optimizer=tf.keras.optimizers.RMSprop( learning_rate=0.01,\n","                                               rho=0.8, momentum=0.01,\n","                                               epsilon=1e-07,\n","                                               centered=False,\n","                                              metrics=['accuracy'])\n","\n","```\n","5. Create a `logsB` directory object and `tensorboard_callbackB` for your `modelB`.\n","\n","6. Train your model on the `X_train` and `y_train`. Assign the results of `historyB = modelB.fit()`. <br>\n","   The rest of the parameters remain except the following: <br>\n","    - Use **`epochs=20`** and add **`batch_size=100`** parameter.\n","    \n","7. Evaluate your `modelB`.\n","   Launch your **`TensorBoard`** to monitor the performance of your `modelB`.\n","    \n","8. Vary your network's number of `Dense` layers and `units` and try to change the `modelB.compile` configuration. <br>   Check how the results change with its variation.\n","\n","10. Compare the outputs of `modelA` and `modelB`. How do you you interpret each of the result of their output layers?     What if the output layer is just a `Dense` layer without activation function?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RxqWbWCgETfv"},"outputs":[],"source":["# 2. Construct the model such that you have the following layers stacked in Sequential manner. Name it modelB.\n","# SC\n","\n","modelB = tf.keras.Sequential([\n","\n","    tf.keras.layers.Dense(..., activation='...', input_shape=[len(X_train[0])]),\n","    tf.keras.layers.Dense(..., activation='...'),\n","    tf.keras.layers.Dense(..., activation='...' )\n","\n","  ])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SauKHm5hETfv"},"outputs":[],"source":["# 3. How many trainable parameters/coefficients do you have?\n","\n","# SC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYmqw1THETfv"},"outputs":[],"source":["# 4. Compile your model using the following hyperparameters\n","\n","# SC\n","modelB.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              optimizer=tf.keras.optimizers.RMSprop(learning_rate=...,\n","                                                    rho=...,\n","                                                    momentum=...,\n","                                                    epsilon=...,\n","                                                    centered=False),\n","               metrics=['...'])\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7yA9XX4ETfv"},"outputs":[],"source":["# 5. Create a new logs_reg directory where you store your training logs for model_reg. Set your tensorboard_callback_reg.\n","\n","# SC\n","\n","... = os.path.join(\"logs\", \"...\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","... = tf.keras.callbacks.TensorBoard(..., histogram_freq=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"Dx5B2sriETfv"},"outputs":[],"source":["# 6.Train your model on the train_data and test_data. The rest of the parameters remain except the following\n","\n","# SC\n","historyB = modelB.fit(X_train, y_train,\n","                    epochs=...,\n","                    batch_size=...,\n","                    validation_data=(X_val, y_val),\n","                    verbose=1,\n","                    callbacks=[...])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"scG6ThPPETfv"},"outputs":[],"source":["# 7. Evaluate your `modelB`.\n","modelB.evaluate(..., ...)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fx_kYA0eETfv"},"outputs":[],"source":["# 8. Compare the outputs of modelA and modelB. How do you you interpret each of the result of their output layers?\n","\n","# SC"]},{"cell_type":"markdown","metadata":{"id":"JZqM7Sy0ETfv"},"source":["**`modelA`**:\n","\n","\n","**`modelB`**:\n","\n","\n","**`Dense without Activation Function`**:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fpxPqXBHETfv"},"outputs":[],"source":["compareValLoss(historyA, historyB, 'modelA', 'modelB')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9OXMc5iETfv"},"outputs":[],"source":["compareValAcc(historyA, historyB, 'modelA', 'modelB')"]},{"cell_type":"markdown","metadata":{"id":"Toa7oOfhETfv"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"cudnAyBUETfw"},"source":["## 5. Improving Neural Network Performance\n","\n","We can further improve our artificial neural network performance. <br>\n","Listed below are some of the commonly-used modification to keep the model from overfitting."]},{"cell_type":"markdown","metadata":{"id":"zOR-Ta6oETfw"},"source":["### 5.1. Adding more training data\n","Deep neural networks perform better with high-volume data."]},{"cell_type":"markdown","metadata":{"id":"pkn_n7x1ETfw"},"source":[" ### 5.2. Reduce  network capacity\n","We saw in the Exercises that the accuracy of the network plateaus at a certain value. Moreover, we see a difference between training and validation performance. When our network has more capacity compared to the amount of training data we have, our network largely overfits our data. Having fewer `units`, like our first neural network may handle the churn data better than our neural network because it may overfit less."]},{"cell_type":"markdown","metadata":{"id":"sU5HMiAeETfw"},"source":["### Recall on Metrics"]},{"cell_type":"markdown","metadata":{"id":"YIfstyzWETfw"},"source":["There are few metrics that can also be computed and are helpful in evaluating model performance.\n","\n","*   **`False Negatives`** and **`False Positives`** are samples that were **incorrectly** classified\n","*   **`True Negatives`** and **`True Positives`** are samples that were **correctly** classified\n","*   **`Accuracy`** is the percentage of examples correctly classified <br>  \n","*   **`Precision`** is the percentage of **predicted** positives that were correctly classified <br>\n","*   **`Recall`** is the percentage of **actual** positives that were correctly classified <br>\n","*   **`AUC`** refers to the Area Under the Curve of a Receiver Operating Characteristic curve (ROC-AUC). This metric is equal to the probability that a classifier will rank a random positive sample higher than than a random negative sample.\n","\n","**NB**: We can have `99.8%+` accuracy on this task by predicting False all the time. Hence, accuracy is not always useful."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTA_bOomETfw"},"outputs":[],"source":["METRICS = [\n","      tf.keras.metrics.TruePositives(name='tp'),\n","      tf.keras.metrics.FalsePositives(name='fp'),\n","      tf.keras.metrics.TrueNegatives(name='tn'),\n","      tf.keras.metrics.FalseNegatives(name='fn'),\n","      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n","      tf.keras.metrics.Precision(name='precision'),\n","      tf.keras.metrics.Recall(name='recall'),\n","      tf.keras.metrics.AUC(name='auc'),\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_Lcq5tZETfw"},"outputs":[],"source":["EPOCHS = 20\n","BATCH_SIZE = 2048"]},{"cell_type":"markdown","metadata":{"id":"DNpCvNehETfw"},"source":["We now build our baseline model inside a function to allow re-usability during experimentation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqpHNkWPETfw"},"outputs":[],"source":["def build_model(metrics = METRICS, output_bias=None):\n","    model = tf.keras.Sequential([\n","            tf.keras.layers.Dense(32, activation='relu', input_shape=[len(train_data.keys())]),\n","            tf.keras.layers.Dense(32, activation='relu'),\n","            tf.keras.layers.Dense(1, activation='sigmoid',\n","                                 bias_initializer=output_bias)\n","             ])\n","\n","    model.compile(\n","          optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n","          loss=tf.keras.losses.BinaryCrossentropy(),\n","          metrics=METRICS)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"MphK3mtiETfw"},"source":["## Exercise 2:\n","\n","Build and name `modelC` using the `build_model` function above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLIQldnrETfw"},"outputs":[],"source":["# SC\n","... = ...\n","... = ...evaluate(X_train, y_train, batch_size=BATCH_SIZE, verbose=0)"]},{"cell_type":"markdown","metadata":{"id":"fUSV7kGZETfw"},"source":["### 5.3. Initializing Output Layer Bias"]},{"cell_type":"markdown","metadata":{"id":"1CB8cIjOETfw"},"source":["We know that our data is **IMBALANCED** and that the initial bias of the network is rarely correct. <br>\n","We can set the initial bias in our `output layer` to reflect the imbalance. This can also help the network reach convergence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x43FhOn3ETfw"},"outputs":[],"source":["init_bias = np.log([count_churn/count_retain])\n","output_bias = tf.keras.initializers.Constant(init_bias)\n","print(\"Initial Bias: {}\".format(init_bias[0]))"]},{"cell_type":"markdown","metadata":{"id":"Ga1lSr5BETfw"},"source":["## Exercise 3:\n","\n","1. Build another model named `modelD`. Make use of the `output_bias` above.\n","2. Compare the loss of `modelC` and `modelD` if the bias initialization helped."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"nDZk2qr0ETfx","executionInfo":{"status":"error","timestamp":1728010157689,"user_tz":-480,"elapsed":426,"user":{"displayName":"Haziq Mudzakir Abdul Wahab","userId":"14421371256827986254"}},"outputId":"69e1f16b-6a2c-498f-a3cf-d6d26d1b804b","colab":{"base_uri":"https://localhost:8080/","height":108}},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"cannot assign to ellipsis here. Maybe you meant '==' instead of '='? (<ipython-input-1-b98ddb594b2f>, line 4)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-b98ddb594b2f>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    ... = build_model(output_bias=output_bias)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to ellipsis here. Maybe you meant '==' instead of '='?\n"]}],"source":["# SC\n","\n","# 1.\n","... = build_model(output_bias=output_bias)\n","... = ...evaluate(X_train, y_train, batch_size=BATCH_SIZE, verbose=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCAw6biNETfx"},"outputs":[],"source":["modelD_eval"]},{"cell_type":"markdown","metadata":{"id":"kNpYQR6qETfx"},"source":["We keep the initial weights in a `checkpoint file`, and load them into each model before training. <br>\n","By doing so we can **compare various training runs**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lexBpdSmETfx"},"outputs":[],"source":["initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\n","modelD.save_weights(initial_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"daUOnVqzETfx"},"outputs":[],"source":["# SC\n","\n","# 2.\n","print(\"modelC Loss:{}\".format(modelC_eval[0]))\n","print(\"modelD Loss:{}\".format(modelD_eval[0]))"]},{"cell_type":"markdown","metadata":{"id":"sdrzZW0lETfx"},"source":["## Exercise 4:\n","\n","1. Build `modelE` using the `initial_weights` above. However, set the `output` layer bias to `0.0`.\n","2. Set-up your `tensorboard_callback` and `logdir` for `modelE`.\n","2. Fit `modelE` and assign it to `zero_bias_history`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1xlb50OETfx"},"outputs":[],"source":["# SC\n","\n","# 1.\n","...= build_model()\n","....load_weights(initial_weights)\n","....layers[-1].bias.assign([0.0])\n","\n","# 2.\n","... = os.path.join(\"logs\", \"...\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","... = tf.keras.callbacks.TensorBoard(..., histogram_freq=1)\n","\n","# 3.\n","zero_bias_history = modelE.fit(\n","    X_train,\n","    y_train,\n","    batch_size=BATCH_SIZE,\n","    epochs=20,\n","    validation_data=(X_val, y_val),\n","    verbose=0,\n","    callbacks = [...] )"]},{"cell_type":"markdown","metadata":{"id":"-penso8IETfx"},"source":["## Exercise 5:\n","\n","1. Build `modelF` using the `initial_weights` above without changing the `output` layer bias.\n","2. Set-up your `tensorboard_callback` and `logdir` for `modelF`.\n","2. Fit `modelF` and assign it to `careful_bias_history`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byLxhAWaETfx"},"outputs":[],"source":["# SC\n","\n","# 1.\n","... = build_model()\n","....load_weights(initial_weights)\n","\n","# 2.\n","... = os.path.join(\"logs\", \"...\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","... = tf.keras.callbacks.TensorBoard(..., histogram_freq=1)\n","\n","# 3.\n","... = ....fit(\n","     X_train,\n","    y_train,\n","    batch_size=BATCH_SIZE,\n","    epochs=20,\n","    validation_data=(X_val, y_val),\n","    verbose=0,\n","    callbacks = [...])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKCfDbjZETfx"},"outputs":[],"source":["compareValLoss(zero_bias_history, careful_bias_history, 'Zero Bias', 'Careful Bias')"]},{"cell_type":"markdown","metadata":{"id":"XWS0YpCXETfx"},"source":["We see from the comparative plot that the model with careful initialization **`modelF`** has smaller `loss`. Let's use it in our actual training.\n","\n","There are other callbacks that we can add to our training.  <br>\n","The **Early Stopping Callback** allows us to stop the training when a monitored quantity has **stopped improving**. <br>\n","The **patience** parameter is the amount of epochs to check for improvement."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTE8UxRnETfx"},"outputs":[],"source":["early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_auc',\n","    verbose=1,\n","    patience=10,\n","    mode='max',\n","    restore_best_weights=True)"]},{"cell_type":"markdown","metadata":{"id":"wiUIysTDETfx"},"source":["## Exercise 6:\n","\n","1. Build `modelG` using the `initial_weights` above without changing the `output` layer bias.\n","2. Set-up your `tensorboard_callback` and `logdir` for `modelF`.\n","2. Fit `modelF` and assign it to `baseline_history`. Don't forget to add `early_stopping` to your `callbacks`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZUnMCAh8ETfx"},"outputs":[],"source":["# SC\n","\n","# 1.\n","... = build_model()\n","....load_weights(initial_weights)\n","\n","# 2.\n","... = os.path.join(\"logs\", \"...\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","... = tf.keras.callbacks.TensorBoard(..., histogram_freq=1)\n","\n","# 3.\n","baseline_history = modelG.fit(\n","    X_train,\n","    y_train,\n","    batch_size=BATCH_SIZE,\n","    epochs=EPOCHS,\n","    validation_data=(X_val, y_val),\n","    verbose=0,\n","    callbacks = [..., ...])"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"F8O4Qx5aETfx"},"outputs":[],"source":["plot_metrics(baseline_history)"]},{"cell_type":"markdown","metadata":{"id":"HO8cncwAETfx"},"source":["### Metrics Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OtWhAcgkETfy"},"outputs":[],"source":["train_predictions_baseline = modelG.predict(X_train, batch_size=BATCH_SIZE)\n","test_predictions_baseline = modelG.predict(X_test, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"tTKcxU2RETfy"},"outputs":[],"source":["baseline_results = modelG.evaluate(X_test, y_test,\n","                                  batch_size=BATCH_SIZE, verbose=0)\n","for name, value in zip(modelG.metrics_names, baseline_results):\n","    print(name, ': ', value)\n","print()\n","\n","plot_cm(y_test, test_predictions_baseline)"]},{"cell_type":"markdown","metadata":{"id":"CQLWet3VETfy"},"source":["The ROC plot shows at a glance the range of performance the model can reach just by tuning the output threshold."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"qgtfJN7oETfy"},"outputs":[],"source":["plot_roc(\"Train Baseline\", y_train, train_predictions_baseline, color=colors[0])\n","plot_roc(\"Test Baseline\", y_test, test_predictions_baseline, color=colors[0], linestyle='--')\n","plt.legend(loc='lower right')"]},{"cell_type":"markdown","metadata":{"id":"UzWcFBW5ETfy"},"source":["<!-- It looks like the precision is relatively high, but the recall and the area under the ROC curve (AUC) aren't as high as you might like. Classifiers often face challenges when trying to maximize both precision and recall, which is especially true when working with imbalanced datasets. It is important to consider the costs of different types of errors in the context of the problem you care about. In this example, a false negative (a fraudulent transaction is missed) may have a financial cost, while a false positive (a transaction is incorrectly flagged as fraudulent) may decrease user happiness. -->\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NC2irnqxETfy"},"source":["### 5.4. Add weights to class"]},{"cell_type":"markdown","metadata":{"id":"UEHR6ETMETfy"},"source":["The goal is to identify churned customers, but you don't have very many of those positive samples to work with, so you would want to have the classifier heavily weight the few examples that are available. You can do this by passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1IaJzh79ETfy"},"outputs":[],"source":["# Scaling by total/2 helps keep the loss to a similar magnitude.\n","# The sum of the weights of all examples stays the same.\n","total = count_retain+count_churn\n","\n","weight_for_0 = (1 / count_retain)*(total)/2.0\n","weight_for_1 = (1 / count_churn)*(total)/2.0\n","\n","class_weight = {0: weight_for_0, 1: weight_for_1}\n","\n","print('Weight for class 0: {:.2f}'.format(weight_for_0))\n","print('Weight for class 1: {:.2f}'.format(weight_for_1))"]},{"cell_type":"markdown","metadata":{"id":"QUtcbc9YETf3"},"source":["Now we try re-training and evaluating the model with weighted class to see how that affects the predictions."]},{"cell_type":"markdown","metadata":{"id":"X6glkSlFETf3"},"source":["## Exercise 7:\n","\n","1. Build `weighted_model` using the `initial_weights` above without changing the `output` layer bias.\n","2. Assign the training of `weighted_model` to `weighted_history`. Don't forget to add `early_stopping` to your `callbacks`. Assign values to `class_weight`.\n","3. Use `plot_metrics` on `weighted_history`\n","4. Use `weighted_model` to predict both on `X_train` and `X_test`\n","5. Evaluate `weighted_model` and assign result to `weighted_results`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3l5qEMgETf3"},"outputs":[],"source":["# SC\n","\n","# 1.\n","... = build_model()\n","....load_weights(initial_weights)\n","\n","# 2.\n","... = ....fit(\n","    X_train,\n","    y_train,\n","    batch_size=BATCH_SIZE,\n","    epochs=EPOCHS,\n","    callbacks = [...],\n","    validation_data=(X_val, y_val),\n","    # The class weights go here\n","    class_weight=class_weight)\n","\n","# 3.\n","plot_metrics(...)\n","\n","# 4.\n","train_predictions_weighted = ....predict(X_train, batch_size=BATCH_SIZE)\n","test_predictions_weighted = ....predict(X_test, batch_size=BATCH_SIZE)\n","\n","# 5.\n","... = ....evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8noAz7JvETf3"},"outputs":[],"source":["for name, value in zip(weighted_model.metrics_names, weighted_results):\n","    print(name, ': ', value)\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LIgQhBs5ETf3"},"outputs":[],"source":["plt.clf()\n","plot_cm(y_test, test_predictions_weighted)\n"]},{"cell_type":"markdown","metadata":{"id":"WDRnauT0ETf3"},"source":["<!-- Here you can see that with weighted class, the accuracy and precision are lower because there are more false positives, but conversely the recall and AUC are higher because the model also found more true positives. Despite having lower accuracy, this model has higher recall (and identifies more fraudulent transactions). Of course, there is a cost to both types of error (you wouldn't want to bug users by flagging too many legitimate transactions as fraudulent, either). Carefully consider the trade offs between these different types of errors for your application. -->"]},{"cell_type":"markdown","metadata":{"id":"3vRcGRThETf3"},"source":["### 5.5. Regularization\n","\n","#### 5.5.1. L1 and L2 Regularization\n","A model is simple if  the distribution of parameter values has **`less entropy`**. This is achieved by having **`fewer parameters`**. Another way to mitigate overfitting is to put constraints on the weights to take only small values, making its value distribution **`more regular`**. This is called **`weight regularization`**. We penalize the network for having large weights.\n","\n","- **`L1 regularization`**: The cost added is proportional to the absolute value of the weight coefficients (the `L1 norm` of the weights).\n","- **`L2 regularization`**: The cost added is proportional to the square of the value of the weight coefficients (the `L2 norm or weight decay`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hX05DextETf3"},"outputs":[],"source":["# assign L1 regularizer\n","L1_reg = regularizers.l1(0.001)\n","\n","# assign L1 regularizer\n","L2_reg = regularizers.l2(0.001)\n","\n","# OR assign L1 and L2 regularizer simultaneously\n","L1L2_reg = regularizers.l1_l2(l1=0.001, l2=0.001)"]},{"cell_type":"markdown","metadata":{"id":"kUql1582ETf3"},"source":["Let's compare the difference in `val_loss` for both baseline and regularized `modelA`."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"5YNfjoc2ETf3"},"outputs":[],"source":[" # Instantiate modelA\n","modelA_L2 = keras.Sequential([\n","    tf.keras.layers.Dense(4, kernel_regularizer = L2_reg, activation='relu', input_shape=[len(X_train[0])]),\n","    tf.keras.layers.Dense(4, kernel_regularizer = L2_reg, activation='relu'),\n","    tf.keras.layers.Dense(1,   activation='sigmoid')\n","\n","  ])\n","\n","# Compile modelA\n","modelA_L2.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","# Train modelA\n","historyA_L2 = modelA_L2.fit(X_train, y_train,\n","                    epochs=20,\n","                    validation_split = 0.2,\n","                    verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"mlGZ_L9mETf3"},"outputs":[],"source":["# Print the model summary\n","modelA_L2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N3Z_HbkLETf3"},"outputs":[],"source":["compareValLoss(historyA, historyA_L2, 'modelA', 'modelA L2 Regularized')"]},{"cell_type":"markdown","metadata":{"id":"XLgZ5os0ETf4"},"source":["We observed that with L2 regularization, validation improved compared to the baseline model."]},{"cell_type":"markdown","metadata":{"id":"LPqKUKtbETf4"},"source":["#### 5.5.2. Adding Drop-out Layer\n","\n","Dropout, applied to a layer, consists of randomly dropping out (setting to zero) a number of output features of the layer during training.\n","\n","We want units to be independently extract features from their inputs instead of relying on other neurons to do so. Suppose we have a multilayered feedforward network like this one, the yellow hidden units in the middle layer may co-adapt.  Subsets of units in a layer is randomly selected and pruned by clamping their output to zero.  This effectively removes those units from the model. A different subset of units is randomly selected every time we present a training example.\n","\n","Below are two possible network configurations. On the first presentation (left), the 1st and 3rd units are disabled, but the 2nd and 3rd units have been randomly selected on a subsequent presentation."]},{"cell_type":"markdown","metadata":{"id":"w0zPnTsUETf4"},"source":["<img src=\"./images/nn_dropout.png\" style=\"height: 200px;\" align=left>"]},{"cell_type":"markdown","metadata":{"id":"t0VodZpoETf4"},"source":["This image was accessed from [StackExchange](https://stats.stackexchange.com/questions/201569/what-is-the-difference-between-dropout-and-drop-connect?noredirect=1&lq=1)."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"tib5YkIRETf4"},"outputs":[],"source":[" # Instantiate modelA\n","modelA_dropout = tf.keras.Sequential([\n","    tf.keras.layers.Dense(4, activation='relu', input_shape=[len(X_train[0])]),\n","    tf.keras.layers.Dropout(0.1),\n","    tf.keras.layers.Dense(4, activation='relu'),\n","    tf.keras.layers.Dropout(0.1),\n","    tf.keras.layers.Dense(1,   activation='sigmoid')\n","\n","  ])\n","\n","# Compile modelA\n","modelA_dropout.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","# Train modelA\n","historyA_dropout = modelA_dropout.fit(X_train, y_train,\n","                    epochs=20,\n","                    validation_split = 0.2,\n","                    verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"rCwZNkLdETf4"},"outputs":[],"source":["# Print the model summary\n","modelA_dropout.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7YXW1fLETf4"},"outputs":[],"source":["compareValLoss(historyA, historyA_dropout, 'modelA', 'modelA Dropout Regularized')"]},{"cell_type":"markdown","metadata":{"id":"Qt4Mwz2HETf4"},"source":["### 5.6. Configuring Learning Rates\n","\n","Shown below how the learning rate can be configured.\n","\n","```python\n","\n","optimizer = tf.keras.optimizers.Adam( learning_rate=0.001 )\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"6nLxMlA3ETf4"},"source":["## Exercise 8:\n","\n","1. Use `ModelA` and use the `optimizer` above. However, use `epochs=5`.\n","2. Vary the step-size as: `learning_rate=0.0001`, `learning_rate=0.01`, `learning_rate=0.8`. Compile the model and assign names to history as `historyA_LRsmall`, `historyA_LRmid`, `historyA_LRbig`, respectively.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwIc1FTlETf4"},"outputs":[],"source":["# SC\n","\n","optimizer = ...\n","\n","# Compile modelA\n","modelA.compile(loss='binary_crossentropy',\n","              optimizer=...,\n","              metrics=['accuracy'])\n","\n","# Train modelA\n","historyA_LRsmall = modelA.fit(X_train, y_train,\n","                    epochs=...,\n","                    batch_size=100,\n","                    validation_data=(X_val, y_val),\n","                    verbose=1,\n","                    callbacks=[tensorboard_callbackA])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gd1VTRE6ETf4"},"outputs":[],"source":["# SC\n","optimizer = ...\n","\n","# Compile modelA\n","modelA.compile(loss='binary_crossentropy',\n","              optimizer=...,\n","              metrics=['accuracy'])\n","\n","\n","# Train modelA\n","historyA_LRmid = modelA.fit(X_train, y_train,\n","                    epochs=...,\n","                    batch_size=100,\n","                    validation_data=(X_val, y_val),\n","                    verbose=1,\n","                    callbacks=[tensorboard_callbackA])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPTq6xUiETf4"},"outputs":[],"source":["# SC\n","optimizer = ...\n","\n","# Compile modelA\n","modelA.compile(loss='binary_crossentropy',\n","              optimizer=...,\n","              metrics=['accuracy'])\n","\n","\n","# Train modelA\n","historyA_LRbig = modelA.fit(X_train, y_train,\n","                    epochs=...,\n","                    batch_size=100,\n","                    validation_data=(X_val, y_val),\n","                    verbose=1,\n","                    callbacks=[tensorboard_callbackA])"]},{"cell_type":"markdown","metadata":{"id":"wOPajydiETf4"},"source":["# References\n","\n","Majority of discussions are based from \"Deep Learning with Python\" by François  Chollet. <br>\n","However, contents are modified to accommodate Keras in Tensorflow 2.0 Backend. <br>\n","Images ours, unless otherwise specified.\n","\n","1. \"True vs. False and Positive vs. Negative\". Machine Learning Crash Course. Feb 10, 2020.  https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative. Accessed 12 April 2020.\n","1. \"Accuracy\". Machine Learning Crash Course. Feb 10, 2020. https://developers.google.com/machine-learning/crash-course/classification/accuracy. Accessed 12 April 2020.\n","\n","1. \"Precision and Recall\". Machine Learning Crash Course. Feb 10, 2020.  https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall. Accessed 12 April 2020.\n","\n","1. \"ROC-AUC\". Machine Learning Crash Course. Feb 10, 2020.   https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc. Accessed 12 April 2020.\n","\n","1. \"Classification on imbalanced data.\" Tensorflow. Apr 04, 2020,  https://www.tensorflow.org/tutorials/structured_data/imbalanced_data. Accessed 12 April 2020.\n","\n","1. Karpathy, Andrej. \"A Recipe for Training Neural Networks.\" Andrej Karpathy blog, Apr 25, 2019, http://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines. Accessed 12 April 2020.\n","\n","1. Chollet, François. Deep Learning with Python. Manning, 2018."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J3WK0ZV-ETf4"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}