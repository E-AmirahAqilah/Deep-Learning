{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cads-logo.png\" style=\"height: 100px;\" align=left> \n",
    "<img src=\"./images/tf-logo-2.png\" style=\"height: 70px;\" align=right>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-requisites:**\n",
    "\n",
    "1. Supervised and Unsupervised Machine Learning\n",
    "2. Linear Alegbra, Calculus\n",
    "3. ```Keras``` Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1. Solving a Single Layer Neural Network](#1.-Solving-a-Single-Layer-Neural-Network)\n",
    "- [2. Parametric Approach to Solving the Neural Network Problem](#2.-Parametric-Approach-to-Solving-the-Single-Layer-Neural-Network)\n",
    "    - [2.1. Weights](#2.1.-Weights)\n",
    "    - [2.2. Bias](#2.2.-Bias)\n",
    "    - [2.3. Activation Function](#2.3.-Activation-Function)\n",
    "    - [2.4. Loss Function](#2.4.-Loss-Function)\n",
    "    - [2.5. Cost Function](#2.5.-Cost-Function)\n",
    "    - [2.6. Gradient Descent](#2.6.-Gradient-Descent)\n",
    "- [3. Single Layer Neural Network from Scratch](#3.-Single-Layer-Neural-Network-from-Scratch)\n",
    "    \n",
    "- [4. Summary of Steps in Training a Single Layer Neural Network for Binary Classification](#4.-Summary-of-Steps-in-Training-a-Single-Layer-Neural-Network-for-Binary-Classification)\n",
    "- [Important Notes](#Important-Notes)\n",
    "    - [Why Need a non-Linear Activation Function?](#Why-Need-a-non-Linear-Activation-Function?)\n",
    "    - [What really are Neural Networks?](#What-really-are-Neural-Networks?)\n",
    "- [5. Building Neural Networks in ```tf.keras```](#5.-Building-Neural-Networks-in-tf.keras)\n",
    "    - [Solve MNIST data using Neural Networks with Hidden Layer](#Solve-MNIST-data-using-Neural-Networks-with-Hidden-Layer)\n",
    "    - [Varying neural networks hyperparameter](#Vary-neural-networks-hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install helper packages\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 9, 5\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "\n",
    "np.random.seed(42) # we need to set the seed for all the random initialization in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Solving a Single Layer Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by solving a simple (**but provocative**) problem if wellbore will collapse based from whether it has surpassed the physical limits (stress, pressure, sand production)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/wellbore.png\" style=\"height: 200px;\" align=left>\n",
    "<img src=\"./images/no_bias.png\" style=\"height: 200px;\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create feature sets (sand production, pore-pressure, thermal stress) and corresponding label (collapse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SC\n",
    "\n",
    "X =  '''Your Code Here'''\n",
    "y = '''Your Code Here'''\n",
    "\n",
    "print dimensions of both X and y\n",
    "\n",
    "print(\"Feature Set shape:\", X.shape)\n",
    "print(\"Feature Set size: \", y.size)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Labels Set shape:\", y.shape)\n",
    "print(\"Labels Set size: \", y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parametric Approach to Solving the Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write down a  function $z(X,W)$ that takes in both the input data $x$, and a set parameters (otherwise known as weights) $w$, and outputs a number $y$ describing wether a person is diabetic or not. \n",
    "\n",
    "What would this $z(X,W)$  look like?\n",
    "\n",
    "$$z  = X.W$$   \n",
    "\n",
    "$$z  = W^{T}X$$\n",
    "\n",
    "$$z= x_{1}w_{1} + x_{2}w_{2} + x_{3}w_{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Weights\n",
    "\n",
    "Imagine these $w$'s as knobs or buttons that we can tweak to correctly predict if a person is diabetic or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assign initial weights\n",
    "\n",
    "weights = np.random.rand(3,1)\n",
    "\n",
    "print(\"Weights shape: \", weights.shape)\n",
    "print(\"Weights size: \", weights.size)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Bias\n",
    "\n",
    "Is a constant term that does not interact with the input data. \n",
    "\n",
    "It instead gives us some data-independet prefences of a class over the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/bias_neuron.png\" style=\"height: 300px;\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$z = X.W + b$$\n",
    "\n",
    "### $$z = W^{T}X + b$$\n",
    "\n",
    "### $$z= x_{1}w_{1} + x_{2}w_{2} + x_{3}w_{3} + b $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SC\n",
    "\n",
    "bias = '''Your Input Here'''\n",
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the parameteric equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SC\n",
    "\n",
    "z = '''Your Code Here'''\n",
    "\n",
    "print(\"z shape: \", z.shape)\n",
    "print(\"z size: \", z.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these numbers make sense? We need a number from the output that is either 1 or 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Activation Function\n",
    "\n",
    "We need another function to squash the results of $X.W$ between 0 and 1. \n",
    "\n",
    "One such function is called the **sigmoid function**.\n",
    "### $$\\sigma_{z} = \\frac{1} {{1+ e^{-z}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$a = \\sigma(z) = \\frac{1} {{1+ e^{-z}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fprop.png\" style=\"height: 300px;\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MC\n",
    "\n",
    "# define the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return '''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = np.linspace(-10, 10, 100)\n",
    "plt.plot(input_, sigmoid(input_), c=\"r\")\n",
    "plt.title(\"Sigmoid Function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SC\n",
    "\n",
    "z = '''Your Code Here'''\n",
    "\n",
    "# let the predicted output be y_hat\n",
    "\n",
    "y_hat = sigmoid(z)\n",
    "print(\"y_hat shape: \", y_hat.shape)\n",
    "print(\"y_hat size: \", y_hat.size)\n",
    "\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(z,  y_hat, 'ko')\n",
    "plt.plot(input_, sigmoid(input_), c=\"r\")\n",
    "plt.title(\"Sigmoid Function and Predicted Y_hat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After letting the neural net predict the output, we compare this output to the actual value. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.4. Loss Function\n",
    "\n",
    "The loss function determines how good our predicted label $a$ with respect to the true label is $y$ for a **single training example**. That means we want to minimize this loss. A usual form we know is half of the square difference between the predicted value $a$ and true value $y$. \n",
    "\n",
    "$$L(a,y) = \\frac{1}{2} (a - y)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DL_12.png\" style=\"height: 300px;\" align=center>\n",
    "<img src=\"./images/DL_13.png\" style=\"height: 300px;\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this form of loss function **doesn't work well** in binary classification problems. \n",
    "\n",
    "A more appropriate form of the loss function is defined by the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$L(\\hat{y},y) =  - ( y\\log{\\hat{y}} + (1 - y)\\log(1 -\\hat{y} ))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This form of a loss function is otherwise known as **Binary Cross-Entropy**\n",
    "\n",
    "#### To minimize $L(a,y)$:\n",
    "#### If y =1: $L(\\hat{y},y) = -\\log{\\hat{y}}$ ; we want $\\hat{y}$ to be LARGE for  $dL\\dashrightarrow decrease$\n",
    "#### If y =0-: $L(\\hat{y},y) = -\\log{(1-\\hat{y})}$ ; we want $\\hat{y}$ to be small for $dL\\dashrightarrow decrease$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define binary cross-entropy/Loss function L\n",
    "def Cross_Entropy(y_hat, y):\n",
    "    if y == 1:\n",
    "        return -np.log(y_hat)\n",
    "    else:\n",
    "        return -np.log(1 - y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cost Function measures how well we are doing on the **ENTIRE TRAINING SET**. This is given by the following equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(W,b) =  - \\frac{1}{m} \\sum_{i=1}^{m} [( y^{(i)}\\log{a^{(i)}} + (1 - y^{(i)})\\log(1 -a^{(i)} ))$$\n",
    "\n",
    "where $m$ denotes the number of training examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By minimizing the **Cost Function** $J(W,b)$, i.e., we fine tune the values of weights $W$ and bias $b$ that help us achieve zero loss. This is what it means by **training the neural network**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that we are solving a optimization problem. An algorithm that solves this problem is called **Gradient Descent**.\n",
    "\n",
    "It finds the **changes** in parameters $W$ and $b$ that minimizes the cost function $J$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```Repeat until convergence:```{\n",
    "\n",
    "$$ w_{i}:=  w_{i} - \\alpha \\frac{dJ(w_{i})}{dw_{i}}$$}\n",
    "\n",
    "$\\alpha$ here is the learning rate. \n",
    "\n",
    "\n",
    "Information is propagated back to the input, hence the term **Backpropagation**. This is the **central algorithm** in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/gdesc2.png\" style=\"height: 300px;\" align=center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and its derivative for later use\n",
    "def derivative_sigmoid(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "# and its derivative, for later use\n",
    "\n",
    "def derivative_Cross_Entropy(y_hat, y):\n",
    "    if y == 1:\n",
    "        return -1/y_hat\n",
    "    else:\n",
    "        return 1 / (1 - y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Single Layer Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "lr = 0.001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for epoch in range(epochs):\n",
    "    random_index = np.arange(X.shape[0])\n",
    "    np.random.shuffle(random_index)\n",
    "    \n",
    "    loss = []\n",
    "    for i in random_index:\n",
    "        Z = weights*X[i] + bias\n",
    "        y_hat = sigmoid(Z) \n",
    "\n",
    "        loss.append(Cross_Entropy(y_hat, y[i]))\n",
    "        \n",
    "        dEdW = '''Your code here'''\n",
    "        dEdb = '''Your code here'''\n",
    "        \n",
    "        weights = '''Your code here'''\n",
    "        bias = '''Your code here'''\n",
    "        \n",
    "    L.append(np.mean(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure\n",
    "plt.figure()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"The Binary Cross-Entropy Values\")\n",
    "plt.plot(L)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary of Steps in Training a Single Layer Neural Network for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feedforward** \n",
    "1. Draw a batch of training examples $X$.\n",
    "2. Dot product of feature matrix $X$ and weight matrix $W$\n",
    "3. Pass the result to an activation function, which in this case is a sigmoid function\n",
    "4. The output from the sigmoid function is the predicted output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation**\n",
    "1. Calculate the cost using the cost function $J$, that is the difference between the $y_{pred}$ and $y_{obs}$\n",
    "2. Find $W$s and $b$ that minimizes the cost by doing Gradient Descent\n",
    "3. Simultaneously update $W$s and $b$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Need a non-Linear Activation Function?\n",
    "\n",
    "The purpose of activation function is to capture non-linearity.\n",
    "\n",
    "Our neural network will collapse to a linear function if we don't use non-linear activation functions. \n",
    "\n",
    "<img src=\"./images/DL_6.png\" style=\"height: 300px;\" align=center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What really are Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### They are a class of functions where we have simplier functions stacked on top of each other in heirarchicala manner (with non-linear functions in between) in order to make a more complex non-linear function.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./images/function_pile.png\" style=\"height: 400px;\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Building Neural Networks in ```tf.keras```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install ```tf.keras``` by running the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```!pip3 install --upgrade tensorflow```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve MNIST data using Neural Networks with Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we’re trying to solve here is to classify grayscale images of handwritten digits (28 × 28 pixels) into their 10 categories (0 through 9). You can think of “solving” MNIST as the “Hello World” of deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# load the MNIST data. See how simple it is to implement in tf.keras\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data() # how easy to split the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# let's see how an mnist image looks like\n",
    "digit = X_test[0]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by dividing we transfrom the pixel values to so it wall from [0,1]\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets build our neural network!\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)), \n",
    "  tf.keras.layers.Dense(128, activation='relu'), \n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/relu_equation.png' style=\"height: 100px;\" align=center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SC\n",
    "\n",
    "# define the relu function\n",
    "def relu(x):\n",
    "    return '''Your Code Here'''\n",
    "\n",
    "# define softmax function\n",
    "def softmax(x):\n",
    "    return '''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = np.linspace(-10, 10, 100)\n",
    "plt.plot(input_, relu(input_), c=\"r\")\n",
    "plt.title(\"ReLU Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = np.linspace(-10, 10, 100)\n",
    "plt.plot(input_, softmax(input_), c=\"r\")\n",
    "plt.title(\"Softmax Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', lr = 0.01, \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=5, batchsize =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Vary neural networks hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SC\n",
    "\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
